# -*- coding: utf-8 -*-
"""diabetes_femic (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1F9w2G4Bf8WLtpwrwzzC_xy5ijEvuShUy

# Introdução

O conjunto de dados que utilizamos em nosso projeto pode ser encontrado no Kaggle: https://www.kaggle.com/uciml/pima-indians-diabetes-database/data

Este conjunto de dados é originalmente do Instituto Nacional de Diabetes e Doenças Digestivas e Renais. O objetivo da Ciência de Dados  é mostrar a importância da etapa da Análise Exploratória mais detalhada do banco de dados para     prever se um paciente tem ou não diabetes com base em certas medidas de diagnóstico incluídas no conjunto de dados. Várias limitações foram impostas ao selecionar essas instâncias em um banco de dados maior. Especificamente, todos os pacientes aqui são mulheres com pelo menos 21 anos de idade de origem indígena Pima.

O conjunto de dados consiste em várias variáveis ​​médicas preditoras e uma variável alvo, resultado. As variáveis ​​preditoras, dados de entrada, incluem o número de gestações que a paciente teve, seu IMC, níveis de glicose , pressão diastólica,Espessura da pele ,  idade.

Considere a variável de resultado com um valor de 1 para Verdadeiro e 0 para Falso.

**Informações sobre os atributos**

* Pregnancies / Gravidez - Mês em que se encontra a gestação
* Glucose / Glicose - Concentração de glicose no plasma duas horas em um teste oral de tolerância à glicose
* Blood Pressure / Pressão sanguínea - Pressão arterial diastólica (mm Hg)
* SkinThickness / Espessura da pele - Tríceps espessura da dobra da pele (mm)
* Isulin / Insulina - Insulina sérica de 2 horas (mu U / ml)
* BMI / IMC - Índice de massa corporal (peso em kg / (altura em m) ^ 2)
* Diabetes Pedigree Function / Função de pedigree do diabéticos - Função pedigree do diabéticos
* Age / Idade 
* Outcome / Resultado - A variável de classe (0 ou 1)

#Importando os modulos
"""

import matplotlib.pyplot as plt
import pandas as pd
import numpy as np 
import sklearn
import seaborn as sns

"""#Carregando o dataset ou dataframe(df) """

from google.colab import drive
drive.mount('/content/drive')

df=pd.read_csv('/content/drive/MyDrive/diabetes.csv')



"""**Qual o tamanho do dataframe(df)**"""

print(f'Linhas: {df.shape[0]}')
print(f'Colunas: {df.shape[1]}')

"""**Qual o tipo dos atributos(colunas do df)**"""

df.dtypes

"""Nosso conjunto de dados contém 768 linhas e 9 colunas, a coluna de **resultado** que são nossos dados de saída, é o que queremos prever é um tipo inteiro,assim com a maioria dos nossos atributos.

#Verificando as primeiras linhas do dataset
"""

df.head(5)

"""Vamos renomear as colunas do nosso dataset para facilitar o trabalho de análise dos dados."""

df.columns = ['Gravidez', 'Glicose', 'Pressão_Sanguinea', 'Espessura_da_pele', 'Insulina', 'IMC',\
              'Função_pedigree_dos_diabeticos', 'Idade', 'Resultado']

df.head(5)

"""Vamos agora identificar e visualizar a correlação entre as variáveis. Considere o coeficiente de correlação:

* +1 = Forte correlação positiva
* 0 = Não há correlação
* -1 = Forte correlação negativa

# Identificando a correlação entre as variáveis utilizando o modulo seaborn
 **Correlação não implica causalidade**
"""

sns.heatmap(df.corr(),annot=True)
plt.figure(figsize=(10,6))

"""#População que deve ser rastreada

Para que o rastreamento tenha boa relação custo-efetividade, é aconselhável que os procedimentos de diagnóstico
sejam realizados em uma população de alto risco, selecionada
de acordo com os escores de questionários como o Finnish
Diabetes Risk Score (FINDRISC) ou o da Associação Americana de Diabetes (American Diabetes Association, ADA), ambos já validados.2,3

Se o rastreamento for realizado sem a utilização prévia
de questionários, devem ser testados indivíduos acima de
45 anos de idade ou, em qualquer idade, pacientes com sobrepeso/obesidade, hipertensão arterial ou história familiar de DM2. Embora um índice de massa corporal (IMC) > 25 kg/m2
 esteja associado a risco aumentado de DM2,
indivíduos de etnia asiática têm esse risco aumentado já
em IMC > 23 kg/m2
 (B).2

**Testes utilizados**

Qualquer um dos testes aplicados no diagnóstico de
DM2 pode ser usado no rastreamento (glicemia de jejum,
glicemia de 2 horas pós-sobrecarga ou hemoglobina glicada [HbA1c]).

Excluindo Colunas de um Dataframe função drop()

Os valores próximos entre a média e mediana explicam a forma simétrica da curva normal de distribuição de probabilidade.
"""

df.describe()

"""E observamos neste gráfico a presença de **outliers** que são dados que se diferenciam drasticamente de todos os outros. Em outras palavras, um outlier é um valor que foge da normalidade e que pode (e provavelmente irá) causar anomalias nos resultados obtidos por meio de algoritmos e sistemas de análise."""

sns.jointplot(x='IMC',y='Espessura_da_pele',data=df, kind='reg' )

"""O **boxplot** ou diagrama de caixa é uma ferramenta gráfica que permite visualizar a distribuição e valores discrepantes (outliers) dos dados, fornecendo assim um meio complementar para desenvolver uma perspectiva sobre o caráter dos dados."""

df.boxplot(column=['IMC','Espessura_da_pele'])
plt.show()

"""Preenchendo valores missing
É melhor substituir zeros por nan, pois depois disso será mais fácil contá-los e os zeros precisam ser substituídos por valores adequados

# Seleção das variaveis de entrada e saída

A seleção das variáveis é uma etapa muito importante, deve ser selecionada aquelas variáveis mais relevantes. Para isso você deve fazer análise de correlações, o feature selection e aplicar um algoritmo de Machine Learning para buscar as variáveis mais relevantes. Como o nosso dataset é um conjunto de dados pequeno e que não possui valores nulos nas variáveis, iremos utilizar todas as variáveis do dataset como dados de entrada, exceto a variavável resultado que será a nossa saída esperada.
"""

# Seleção de variáveis preditoras (Feature Selection)
atributos = ['Gravidez', 'Glicose', 'Pressão_Sanguinea','Função_pedigree_dos_diabeticos',\
            'Insulina', 'Espessura_da_pele','IMC', 'Idade']

"""Agora iremos selecionar a variável Target, aquilo que queremos prever:"""

# Variável a ser prevista
atrib_prev = ['Resultado']

"""Transformando dados de entrada em X e dados de saída em Y

Vamos agora transformar os nossos dados de entrada em X e os dados de saída, o que queremos prever em Y. X é o nosso input e Y o nosso output, o que queremos que o algoritmo descubra é representado pela função G = X -> Y.
"""

# Criando objetos
X = df[atributos].values
Y = df[atrib_prev].values

"""#Divisão dos dados em Treino e Teste
O que iremos fazer agora é uma divisão do nosso conjunto de dados, após os dados terem sido preparados. Parte desses dados irão para o dataset de treino e a outra parte para o dataset de teste. Normalmente se utiliza a divisão 70% para treino e 30% para teste.

O dataset de treino será utilizado para treinar o nosso modelo e algoritmo. O dataset de teste será utilizado para avaliar o modelo e fazer as previsões.

Existem diversas formas de fazer uma divisão do dataset em treino e teste. A divisão que iremos fazer será utilizando a biblioteca sklearn com a função train_test_split do pacote model_selection.
"""

from sklearn.model_selection import train_test_split

# Definindo a taxa de split
split_test_size = 0.30

"""Vamos chamar agora a função train_test_split, passando os valores de X, Y e dizendo qual será o tamanho do conjunto de teste, neste caso, 0.30 (30%), automaticamente, 70% será para o conjunto de treino.

A função também irá retornar quatro valores, sendo eles para X_treino, X_teste, Y_treino e Y_teste.
"""

# Criando dados de treino e de teste
X_treino, X_teste, Y_treino, Y_teste = train_test_split(X, Y, test_size = split_test_size, random_state = 42)

# Imprimindo os resultados
print(f" {(len(X_treino)/len(df.index) * 100):.2f}% nos dados de treino")
print(f"{(len(X_teste)/len(df.index) * 100): .2f}% nos dados de teste")

"""# **Valores Missing (Ausentes) Ocultos**"""

# Verificando se existem valores nulos
df.isnull().values.any()

"""Vamos agora identificar quantos valores 0 existem em cada coluna."""

print(f"Linhas no dataframe {len(df)}")
print(f"Linhas missing em Glicose: {len(df.query('Glicose == 0'))}")
print(f"Linhas missing em Pressão_Sanguinea: {len(df.query('Pressão_Sanguinea == 0'))}")
print(f"Linhas missing em Espessura_da_pele: {len(df.query('Espessura_da_pele == 0'))}")
print(f"Linhas missing em Insulina: {len(df.query('Insulina == 0'))}")
print(f"Linhas missing em IMC: {len(df.query('IMC == 0'))}")
print(f"Linhas missing em Idade: {len(df.query('Idade == 0'))}")

"""Agora podemos identificar quantos registro de valores 0 temos em cada coluna

**Tratando Dados Missing - Impute**

Existem diversas formas que podemos utilizar para tratar esses valores ausentes, uma delas é utilizando a função **SimpleImputer** do pacote impute da biblioteca **sklearn**

Substituindo os valores iguais a zero pela média dos dados
"""

from sklearn.impute import SimpleImputer

"""Agora vamos informar que os missing_values são todos aqueles igual a 0 e a estrategia é substituir pela média"""

# Criando objeto
preenche_0 = SimpleImputer(missing_values = 0, strategy = "mean")

# Substituindo os valores iguais a zero, pela média dos dados
X_treino = preenche_0.fit_transform(X_treino)
X_teste = preenche_0.fit_transform(X_teste)

"""#Construindo um modelo com o algoritmo de classificação, GaussianNB
A primeira coisa que temos que fazer é selecionar o algoritmo que iremos utilizar. O algoritmo que vamos usar é o algoritmo de classificação **GaussianNB**, um algoritmo e modelo de classificação totalmente baseado no **teorema de bayes**.

Devemos importar o algoritmo (função) GaussianNB do pacote **naive_bayes** da biblioteca sklearn.
"""

# Utilizando um classificador Naive Bayes
from sklearn.naive_bayes import GaussianNB

"""Agora vamos criar uma estância, nosso modelo propriamente dito."""

# Criando o modelo preditivo
modelo_v1 = GaussianNB()

"""Com o nosso modelo criado, vamos agora treinar o nosso modelo. Para isso, vamos utilizar o método fit e apresentar o meu conjunto de dados de treino, **X_treino** e **Y_treino** e o **método ravel**, para ajustar o shape do objeto e conseguir apresentar o algoritmo."""

# Treinando o modelo
modelo_v1.fit(X_treino, Y_treino.ravel())

"""Testando a precisão do modelo

Primeiro vamos importar o pacote metrics da biblioteca sklearn
"""

from sklearn import metrics

"""Verificando a exatidão do modelo nos dados de treino:

Vamos agora chamar o método predict para o modelo e passar como parâmetro apenas as variáveis preditoras (Atributos) de treino, X_treino, para se fazer as previsões do modelo.
"""

nb_predict_train = modelo_v1.predict(X_treino)

"""Com as previsões feitas, vamos verificar a acurácia do nosso modelo utilizando o método **accuracy_score** de metrics, passando como parâmetro **Y_teste** (dados de saída) e **nb_predict_train** (as previsões feitas pelo modelo a partir dos dados de entrada)."""

print(f"Exatidão (Accuracy): {metrics.accuracy_score(Y_treino, nb_predict_train): .2f}")

"""Se observado, para verificar a acurácia estamos utilizando as previsões do modelo(nb_predict_train) a partir dos dados de entrada X_treino com os dados de saída (Y_treino), a previsão esperada.

Quando comparamos a saída esperada (Y_treino) com todas as previsões dos dados de entrada(nb_predict_train) do modelo, obtemos uma taxa de erro, o papel do cientista de dados é diminuir essa taxa de erro e apresentar uma boa exatidão.

Verificando a exatidão do modelo nos **dados de teste**:
"""

nb_predict_test = modelo_v1.predict(X_teste)

print(f"Exatidão (Accuracy): {metrics.accuracy_score(Y_teste, nb_predict_test): .2f}")

"""Se observamos, a acurácia teve uma diminuição devido que os dados de teste são dados que o meu modelo nunca tinha visto antes.

O nosso modelo já tinha visto os dados de treino, porque foi treinado com eles, por outro lado, nunca tinha visto os dados de teste. Isso é importante para identificarmos o nível de acurácia do nosso modelo, porque não adianta ficar medindo a acurácia com dados que ele já conhece, o correto é medir a acurácia com novos dados, com dados de teste em vez de dados de treino.

A acurácia nada mais é do que comparar a saída esperada com as saídas que foram previstas pelo nosso modelo. Normalmente, leva um tempo para se atingir a acurácia ideal, então deve-se continuar trabalhando, buscando outros algoritmos, preparando os dados, para atingir a acurácia ideal.

Vamos agora criar uma confusion matrix, para visualizar melhor o desempenho do nosso modelo.
"""

# Criando uma Confusion Matrix
print("Confusion Matrix")

print(f"{(metrics.confusion_matrix(Y_teste, nb_predict_test, labels = [1, 0]))}")
print('\n')

print("Classification Report")
print(metrics.classification_report(Y_teste, nb_predict_test, labels = [1, 0]))

"""# Otimização, construindo um novo modelo com o algoritmo de Regressão Logística, LogisticRegression
O processo de otimização dentre todos os modelos que serão criados, busca o melhor modelo que resolva o problema em questão.

Iremos criar um novo modelo, um modelo com algoritmo de Regressão Logística para saber se com esse novo algoritmo teremos uma melhor exatidão, acurácia. Para o novo modelo utilizaremos o algoritmo (função) LogistcRegression de regressão logística a partir do pacote linear_model da biblioteca sklearn.
"""

from sklearn.linear_model import LogisticRegression

"""Agora vamos criar a estância do modelo_v2 e passar os parâmetros C, random_state."""

#Segunda versão do modelo usando Regressão Logística
modelo_v2 = LogisticRegression(C = 0.7, random_state = 42)

"""Vamos refazer o que fizemos anteriormente. Treinar o modelo com a função fit e fazer as previsões do modelo com o conjunto de dados de teste a partir dos dados de entrada, para verificarmos a acurácia"""

modelo_v2.fit(X_treino, Y_treino.ravel())

lr_predict_test = modelo_v2.predict(X_teste)

print(f"Exatidão (Accuracy): {metrics.accuracy_score(Y_teste, lr_predict_test): .2f}")
print('\n')
print("Classification Report")
print(metrics.classification_report(Y_teste, lr_predict_test, labels = [1, 0]))

"""Observa-se que temos 0.75 (75%) de acurácia com o modelo de regressão logistica.

Exatidão dos modelos

* Modelo usando o algoritmo de Classificação GaussianNB = 0.74
* Modelo usando o algoritmo de Regressão Logística LogisticRegression = 0.75

# Fazendo Previsões com o modelo treinado
Antes de fazer as previsões, precisamos salvar o modelo, então iremos importar o pacote pickle
"""

import pickle

# Salvando o modelo para usar mais tarde
filename = 'modelo_treinado_v2.sav'
pickle.dump(modelo_v2, open(filename, 'wb'))

"""Agora vamos importar o modelo que foi salvo, utilizando a função load do pacote pickle."""

# Carregando o modelo e fazendo previsão com novos conjuntos de dados 
# (X_teste, Y_teste devem ser novos conjuntos de dados preparados com o procedimento de limpeza e transformação adequados)
loaded_model = pickle.load(open(filename, 'rb'))

"""# Revisando os dados de entrada e saída a ser prevista
Vamos relembrar quem são os nossos dados de entrada, atributos:
"""

print(f'Os dados de entrada são: {atributos}')

"""Todos esses atributos são nossos fatores decisivos para saber se uma pessoa tem diabete ou não, com base em nosso conjunto de dados.

Vamos relembrar quem são os nossoss dados de saída, o que queremos prever:

# Prevendo se a grávida de Monte Carmelo tem ou não diabete
O modelo que vamos utilizar é o modelo que foi feito com o algoritmo LogisticRegression de Regressão Logistica, por obter uma maior acurácia e precisão.

Vamos imaginar que a grávida de Monte Carmelo tenha as seguintes características:

* Gravidez(Gestação) = 9
* Glicose = 171
* Pressão_sanguinea = 110
* Espessura_da_pele = 24
* Insulina = 240
* IMC = 45.4
* Função_p_dos_diabeticos = 0.721
* Idade = 54

Como vocês podem observar, ela está no seu nono mês de gestação, tem uma glicose de 171, uma pressão sanguínea de 110...E tem 54 anos. Essas características serão os nossos dados de entrada para prever se ela tem diabete ou não.

Vamos criar um vetor passando como dados de entrada os valores correspondentes a grávida de taubaté.
"""

Gravida_MonteCarmelo = np.array([9, 171,	110,	24,	240, 45.4,0.721, 54]).reshape((1, -1))

"""Agora vamos usar o **método predict** para saber se a grávida de Monte Carmelo tem ou não diabete."""

#1 = Verdadeiro/Tem diabete
#0 = Falso/Não tem diabete
print(f'Dados de saída (output): {loaded_model.predict(Gravida_MonteCarmelo)[0]}')

"""De acordo com o nosso modelo e os dados de entrada que usamos a grávida de taubaté teria diabete. Como prova disso, os dados de entrada da grávida de Monte carmelo são semelhantes ao do índice 43 do nosso conjunto de dados e a coluna resultado também apresenta o registro 1. Veja:"""

df.query('index == 43')